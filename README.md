# TimeGPT Intraday v2

## Quickstart (≈5 minutes)

1. **Install toolchain** (creates the `.venv`, installs dependencies, and registers pre-commit hooks):

   ```bash
   make install
   ```

2. **Validate the codebase** before making changes:

   ```bash
   make lint
   make test
   ```

3. **Run the end-to-end demo pipeline** on the bundled configs:

   ```bash
   RUN_ID=dev
   python -m timegpt_v2.cli check-data --config-dir configs --run-id "$RUN_ID"
   python -m timegpt_v2.cli build-features --config-dir configs --run-id "$RUN_ID"
   python -m timegpt_v2.cli forecast --config-dir configs --run-id "$RUN_ID" --api-mode offline
   python -m timegpt_v2.cli backtest --config-dir configs --run-id "$RUN_ID"
   python -m timegpt_v2.cli evaluate --config-dir configs --run-id "$RUN_ID"
   python -m timegpt_v2.cli report --config-dir configs --run-id "$RUN_ID"
   ```

   Generated artifacts (features, forecasts, trades, metrics, reports) land under
   `artifacts/runs/<run_id>/...` for inspection.

4. **Explore parameter sweeps** using the trading grid or forecast configuration sweeps:

   ```bash
   # Trading parameter sweep (k_sigma, s_stop, s_take)
   python -m timegpt_v2.cli sweep --config-dir configs --run-id "$RUN_ID" --api-mode offline

   # Forecast configuration sweep (snapshot presets, horizons, quantiles, calibration)
   make forecast-grid-plan  # dry-run plan
   make forecast-grid       # execute with baseline reuse
   ```

   **Note on API Usage:** The `forecast` and `sweep` commands now support `--api-mode`. Use `--api-mode offline` for running sweeps and backtests without hitting the TimeGPT API, relying solely on cached forecasts. To refresh the cache, run `forecast` with `--api-mode online`.

**Exogenous Features:**
The pipeline now supports exogenous features, which can be configured in `configs/forecast.yaml`.
The `exog` section allows you to enable/disable the pipeline, set the strictness level, and declare historical and future exogenous features.
You can also override these settings using the following CLI flags in the `forecast` command:
-   `--use-exogs/--no-exogs`
-   `--strict-exog/--no-strict-exog`
-   `--exog-name-map`

**Common Errors & Fixes:**
-   **`ValueError: Missing historical/future exogenous features`**: This error occurs in strict mode when a declared exogenous feature is not found in the feature matrix.
    -   **Fix:** Ensure that the feature matrix contains all the declared exogenous features, or run in permissive mode (`--no-strict-exog`).
-   **`KeyError: 'feature_name'`**: This can happen if the feature name is misspelled in the configuration or if the feature is not generated by the feature engineering pipeline.
    -   **Fix:** Check the spelling of the feature name in `configs/forecast.yaml` and make sure it is present in the output of the `build-features` command.

- Configuration files live in `configs/` and are the single source of truth for universe definitions,
  scheduler windows, trading rules, backtest aggregation, and forecast grid specifications.
- The system now includes automatic **request-size controls** to prevent "payload too large" errors from the TimeGPT API. It dynamically batches symbols and can partition large requests using `num_partitions`, which may affect API usage counts.
- `docs/` contains deeper design notes on data quality, feature engineering, forecasting, trading, evaluation, and system architecture.
- The `Makefile` mirrors the automation used in CI; running `make fmt && make lint && make test`
  before commits keeps the project reproducible.
- Forecast grid sweeps (Sprint 5) allow systematic exploration of snapshot presets, horizons, quantile sets, target scaling modes, and calibration methods via `configs/forecast_grid.yaml`.

## Accessing GCS Data (local mount)

For the pilot run we mapped the production bucket `gs://jwss_data_store` into the workspace via the
local mount `~/gcs-mount/`. The `check-data` command reads the bronze layer by default using:

- Bucket path: `~/gcs-mount/bronze`
- Template: `stocks/1m/{ticker}/{yyyy}/{ticker}_{yyyy-mm}.parquet`

If your mount or tier differs, adjust `configs/data.yaml` accordingly. The loader uses column aliasing
so raw parquet schemas with `t/o/h/l/c/v` are normalized automatically; only real-time (`session=regular`)
bars between 09:30–16:00 ET are kept.
